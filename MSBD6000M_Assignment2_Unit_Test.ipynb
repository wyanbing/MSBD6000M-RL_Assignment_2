{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EOYXbZKeRajx",
        "outputId": "d80d92e6-2a3f-49c6-bc5d-236acdd6da1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf_agents\n",
            "  Downloading tf_agents-0.19.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (3.1.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf_agents)\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from tf_agents) (11.2.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (1.17.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (5.29.4)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from tf_agents) (1.17.2)\n",
            "Collecting typing-extensions==4.5.0 (from tf_agents)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pygame==2.1.3 (from tf_agents)\n",
            "  Downloading pygame-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting tensorflow-probability~=0.23.0 (from tf_agents)\n",
            "  Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym<=0.23.0,>=0.17.0->tf_agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.23.0->tf_agents) (0.1.9)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability~=0.23.0->tf_agents) (25.3.0)\n",
            "Downloading tf_agents-0.19.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygame-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697739 sha256=0990dac5a1c444713ef3522d5d4b64c6ca7cdbfc8754002da56c44cff4c591aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/19/ce/d2b762b6d61115bf0b4260ca59650ba2d55d49f34f61e095f6\n",
            "Successfully built gym\n",
            "Installing collected packages: typing-extensions, pygame, gym, tensorflow-probability, tf_agents\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.1\n",
            "    Uninstalling pygame-2.6.1:\n",
            "      Successfully uninstalled pygame-2.6.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.25.0\n",
            "    Uninstalling tensorflow-probability-0.25.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.25.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic-core 2.33.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic 2.11.4 requires typing-extensions>=4.12.2, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "sqlalchemy 2.0.40 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "langchain-core 0.3.56 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "altair 5.5.0 requires typing-extensions>=4.10.0; python_version < \"3.14\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "openai 1.76.2 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "google-genai 1.13.0 requires typing-extensions<5.0.0,>=4.11.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typeguard 4.4.2 requires typing_extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typing-inspection 0.4.0 requires typing-extensions>=4.12.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.23.0 pygame-2.1.3 tensorflow-probability-0.23.0 tf_agents-0.19.0 typing-extensions-4.5.0\n",
            "Collecting tensorflow-probability==0.24.0\n",
            "  Downloading tensorflow_probability-0.24.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (2.0.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (3.1.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.24.0) (0.1.9)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability==0.24.0) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability==0.24.0) (1.17.2)\n",
            "Downloading tensorflow_probability-0.24.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-probability\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.23.0\n",
            "    Uninstalling tensorflow-probability-0.23.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-agents 0.19.0 requires tensorflow-probability~=0.23.0, but you have tensorflow-probability 0.24.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-probability-0.24.0\n",
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tf-keras) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tf_agents\n",
        "!pip install \"tensorflow-probability==0.24.0\"\n",
        "!pip install tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "PPO training for Super Tic-Tac-Toe with:\n",
        "- legal action masking\n",
        "- reward shaping\n",
        "- parallel environments\n",
        "- mixed opponent policies\n",
        "\"\"\"\n",
        "\n",
        "import random, time, pathlib, math\n",
        "import numpy as np, matplotlib.pyplot as plt, tqdm.auto as tqdm\n",
        "import tensorflow as tf, tensorflow_probability as tfp\n",
        "from tf_agents.environments  import py_environment, tf_py_environment, parallel_py_environment\n",
        "from tf_agents.specs         import array_spec\n",
        "from tf_agents.trajectories  import time_step as ts, policy_step, from_transition\n",
        "from tf_agents.networks      import network\n",
        "from tf_agents.agents.ppo    import ppo_agent\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.policies      import py_policy\n",
        "from tf_agents.system        import multiprocessing as tf_mp\n",
        "try:\n",
        "    tf_mp.enable_interactive_mode()\n",
        "except ValueError:\n",
        "    pass\n",
        "from tf_agents.policies import greedy_policy\n",
        "\n",
        "# ─── GPU configuration ───────────────────────────────────────────────\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for g in gpus:\n",
        "        tf.config.experimental.set_memory_growth(g, True)\n",
        "    print(f\"{len(gpus)} GPU(s) memory-growth enabled\")\n",
        "\n",
        "# ═════════════ Board geometry and constants ══════════════════════════════\n",
        "BOARD = 12\n",
        "EMPTY, X, O = 0, 1, -1\n",
        "\n",
        "# Create cross-shaped mask over 12×12 grid\n",
        "CROSS = np.zeros((BOARD, BOARD), bool)\n",
        "C0 = BOARD // 2 - 2\n",
        "CROSS[C0:C0+4, C0:C0+4] = True\n",
        "CROSS[:C0,   C0:C0+4] = True\n",
        "CROSS[C0+4:, C0:C0+4] = True\n",
        "CROSS[C0:C0+4, :C0]   = True\n",
        "CROSS[C0:C0+4, C0+4:] = True\n",
        "\n",
        "# Map between flat action index and board coordinates\n",
        "COORDS    = [(r, c) for r in range(BOARD) for c in range(BOARD) if CROSS[r, c]]\n",
        "IDX2COORD = np.array(COORDS, int)\n",
        "N_CELLS   = len(COORDS)\n",
        "\n",
        "# Eight neighbor offsets for random move distribution\n",
        "NEIGH8  = [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]\n",
        "# Precomputed linear indices for TensorFlow masking\n",
        "IDX_LIN = tf.constant([r * BOARD + c for r, c in COORDS], tf.int32)\n",
        "\n",
        "def in_bounds(r, c):\n",
        "    \"\"\"Check if (r, c) is within the BOARD and part of CROSS.\"\"\"\n",
        "    return 0 <= r < BOARD and 0 <= c < BOARD\n",
        "\n",
        "def _count_line(board, r, c, p):\n",
        "    \"\"\"\n",
        "    Count contiguous pieces of player p through (r,c) in 4 directions:\n",
        "    horizontal, vertical, main diagonal, anti-diagonal.\n",
        "    Returns list of counts for each direction.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for dr, dc in [(0,1), (1,0), (1,1), (-1,1)]:\n",
        "        cnt = 1\n",
        "        # forward direction\n",
        "        rr, cc = r + dr, c + dc\n",
        "        while in_bounds(rr, cc) and CROSS[rr, cc] and board[rr, cc] == p:\n",
        "            cnt += 1\n",
        "            rr += dr; cc += dc\n",
        "        # backward direction\n",
        "        rr, cc = r - dr, c - dc\n",
        "        while in_bounds(rr, cc) and CROSS[rr, cc] and board[rr, cc] == p:\n",
        "            cnt += 1\n",
        "            rr -= dr; cc -= dc\n",
        "        out.append(cnt)\n",
        "    return out\n",
        "\n",
        "# ═════════════ Reward shaping constants ══════════════════════════════\n",
        "SCALE       = 1.0\n",
        "R_LEGAL     = 0.5    # base reward for a legal move\n",
        "B_TWO       = 0.5    # bonus for making a 2-in-a-row\n",
        "B_THREE     = 0.8    # bonus for making 3-in-a-row or more\n",
        "R_ILLEGAL   = -1.0   # penalty for illegal move\n",
        "R_PASS_EDGE = -1.0   # penalty when random adjacent picks out-of-bounds\n",
        "P_NO_SPACE  = -1.0   # penalty if no empty neighbors after placement\n",
        "P_IGN_THR   = -0.7   # penalty for ignoring opponent's 3-in-a-row threat\n",
        "\n",
        "# ═════════════ Environment definition ════════════════════════════════\n",
        "class SuperTicTacToe(py_environment.PyEnvironment):\n",
        "    \"\"\"Custom PyEnvironment for the Super Tic-Tac-Toe game.\"\"\"\n",
        "    def __init__(self, opponent=\"random\"):\n",
        "        \"\"\"\n",
        "        Initialize environment specs, opponent policies, and state.\n",
        "        opponent: \"random\", \"rule\", or \"mixed\".\n",
        "        \"\"\"\n",
        "        self._obs_spec = array_spec.BoundedArraySpec((BOARD, BOARD, 3), np.float32, 0, 1)\n",
        "        self._act_spec = array_spec.BoundedArraySpec((), np.int32, 0, N_CELLS - 1)\n",
        "        self._opp_type = opponent\n",
        "        self._rnd  = RandomPolicy(self)\n",
        "        self._rule = RuleBasedPolicy(self)\n",
        "        self._state = None\n",
        "        self._done  = False\n",
        "        self._turn  = X\n",
        "\n",
        "    def observation_spec(self):\n",
        "        \"\"\"Return the observation spec.\"\"\"\n",
        "        return self._obs_spec\n",
        "\n",
        "    def action_spec(self):\n",
        "        \"\"\"Return the action spec.\"\"\"\n",
        "        return self._act_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"Reset the board state and turn. Return initial TimeStep.\"\"\"\n",
        "        self._state = np.zeros((BOARD, BOARD), np.int8)\n",
        "        self._turn  = X\n",
        "        self._done  = False\n",
        "        return ts.restart(self._obs())\n",
        "\n",
        "    def _step(self, action):\n",
        "        \"\"\"\n",
        "        Apply agent move, check terminal, then apply opponent move,\n",
        "        and return the appropriate TimeStep.\n",
        "        \"\"\"\n",
        "        if self._done:\n",
        "            return self.reset()\n",
        "\n",
        "        # Agent's move\n",
        "        reward = self._apply_move(action, self._turn, agent_move=True)\n",
        "        done, winner = self._check_terminal()\n",
        "        if done:\n",
        "            return ts.termination(self._obs(), self._final_reward(winner))\n",
        "\n",
        "        # Select opponent action\n",
        "        opp_pol = self._rule if self._opp_type == \"rule\" else self._rnd\n",
        "        if self._opp_type == \"mixed\":\n",
        "            opp_pol = self._rule if random.random() < 0.3 else self._rnd\n",
        "        opp_act = opp_pol.action(self._fake()).action\n",
        "\n",
        "        # Opponent's move\n",
        "        self._apply_move(int(opp_act), self._turn, agent_move=False)\n",
        "        done, winner = self._check_terminal()\n",
        "        if done:\n",
        "            return ts.termination(self._obs(), self._final_reward(winner))\n",
        "\n",
        "        # Continue with transition\n",
        "        return ts.transition(self._obs(), reward, discount=1.0)\n",
        "\n",
        "    def _apply_move(self, idx, player, agent_move=True):\n",
        "        \"\"\"\n",
        "        Attempt to place piece for player at IDX2COORD[idx].\n",
        "        With 50% chance place at target, else random empty neighbor.\n",
        "        Returns shaped reward or penalty.\n",
        "        \"\"\"\n",
        "        r, c = IDX2COORD[int(idx)]\n",
        "        # Illegal: occupied or outside CROSS\n",
        "        if self._state[r, c] != EMPTY or not CROSS[r, c]:\n",
        "            if agent_move:\n",
        "                # switch turn on illegal agent move\n",
        "                self._turn = -player\n",
        "                return R_ILLEGAL\n",
        "            else:\n",
        "                return 0.0\n",
        "\n",
        "        # 50% chance to place at chosen spot\n",
        "        if random.random() < 0.5:\n",
        "            self._state[r, c] = player\n",
        "            raw = self._positional_reward(r, c, player) if agent_move else 0.0\n",
        "            self._turn = -player\n",
        "            return raw * SCALE\n",
        "\n",
        "        # Otherwise choose random legal neighbor\n",
        "        legal = [\n",
        "            (r+dr, c+dc)\n",
        "            for dr, dc in NEIGH8\n",
        "            if in_bounds(r+dr, c+dc)\n",
        "            and CROSS[r+dr, c+dc]\n",
        "            and self._state[r+dr, c+dc] == EMPTY\n",
        "        ]\n",
        "        if not legal:\n",
        "            self._turn = -player\n",
        "            return (R_PASS_EDGE * SCALE) if agent_move else 0.0\n",
        "\n",
        "        rr, cc = random.choice(legal)\n",
        "        self._state[rr, cc] = player\n",
        "        raw = self._positional_reward(rr, cc, player) if agent_move else 0.0\n",
        "        self._turn = -player\n",
        "        return raw * SCALE\n",
        "\n",
        "    def _positional_reward(self, r, c, p):\n",
        "        \"\"\"\n",
        "        Compute shaped reward after placing at (r,c) by player p:\n",
        "        - base legal move reward\n",
        "        - bonus for 2-in-row or 3+-in-row\n",
        "        - penalty if no empty neighbors\n",
        "        - penalty if ignoring opponent threat\n",
        "        \"\"\"\n",
        "        bonus = R_LEGAL\n",
        "\n",
        "        # Bonus for creating lines\n",
        "        own_lengths = _count_line(self._state, r, c, p)\n",
        "        m = max(own_lengths)\n",
        "        if m >= 3:\n",
        "            bonus += B_THREE\n",
        "        elif m == 2:\n",
        "            bonus += B_TWO\n",
        "\n",
        "        # Penalty if no empty adjacent cells\n",
        "        no_space = True\n",
        "        for dr, dc in NEIGH8:\n",
        "            rr, cc = r + dr, c + dc\n",
        "            if in_bounds(rr, cc) and CROSS[rr, cc] and self._state[rr, cc] == EMPTY:\n",
        "                no_space = False\n",
        "                break\n",
        "        if no_space:\n",
        "            bonus += P_NO_SPACE\n",
        "\n",
        "        # Penalty if opponent has an active 3-in-row threat\n",
        "        if self._opponent_threat():\n",
        "            bonus += P_IGN_THR\n",
        "\n",
        "        return bonus\n",
        "\n",
        "    def _opponent_threat(self):\n",
        "        \"\"\"\n",
        "        Check if opponent has any 3-in-row threat or live-2 threat\n",
        "        not yet blocked on the board.\n",
        "        \"\"\"\n",
        "        for r, c in COORDS:\n",
        "            if self._state[r, c] != -self._turn:\n",
        "                continue\n",
        "            lengths = _count_line(self._state, r, c, -self._turn)\n",
        "            if max(lengths) >= 3:\n",
        "                return True\n",
        "            # Live-2 horizontal threat\n",
        "            if lengths[0] == 2:\n",
        "                left  = (r, c-1)\n",
        "                right = (r, c+1)\n",
        "                if all(in_bounds(rr, cc) and CROSS[rr, cc] and self._state[rr, cc] == EMPTY\n",
        "                       for rr, cc in [left, right]):\n",
        "                    return True\n",
        "            # Live-2 vertical threat\n",
        "            if lengths[1] == 2:\n",
        "                up   = (r-1, c)\n",
        "                down = (r+1, c)\n",
        "                if all(in_bounds(rr, cc) and CROSS[rr, cc] and self._state[rr, cc] == EMPTY\n",
        "                       for rr, cc in [up, down]):\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    def _check_terminal(self):\n",
        "        \"\"\"\n",
        "        Check if game is won or board is full.\n",
        "        Returns (done_flag, winner) where winner ∈ {X, O, 0}.\n",
        "        \"\"\"\n",
        "        for r, c in COORDS:\n",
        "            p = self._state[r, c]\n",
        "            if p == EMPTY:\n",
        "                continue\n",
        "            lengths = _count_line(self._state, r, c, p)\n",
        "            # Win: 4 in row/col or 5 on diagonal\n",
        "            if (max(lengths) >= 4 and any(lengths[:2])) or max(lengths) >= 5:\n",
        "                return True, p\n",
        "        # Draw if no empty cells\n",
        "        if np.all(self._state[CROSS] != EMPTY):\n",
        "            return True, 0\n",
        "        return False, None\n",
        "\n",
        "    @staticmethod\n",
        "    def _final_reward(w):\n",
        "        \"\"\"\n",
        "        Compute final reward: +3 for X win, -3 for O win, 0 for draw.\n",
        "        \"\"\"\n",
        "        base = 3.0 if w == X else -3.0 if w == O else 0.0\n",
        "        return base * SCALE\n",
        "\n",
        "    def _obs(self):\n",
        "        \"\"\"\n",
        "        Build observation tensor of shape (BOARD,BOARD,3):\n",
        "        channel 0: X positions, channel 1: O positions,\n",
        "        channel 2: current player flag.\n",
        "        \"\"\"\n",
        "        x = (self._state == X).astype(np.float32)\n",
        "        o = (self._state == O).astype(np.float32)\n",
        "        p = np.full_like(x, 1.0 if self._turn == X else 0.0, np.float32)\n",
        "        return np.stack([x, o, p], -1)\n",
        "\n",
        "    def _fake(self):\n",
        "        \"\"\"\n",
        "        Generate a fake TimeStep for opponent policy calls,\n",
        "        preserving observation shape.\n",
        "        \"\"\"\n",
        "        return ts.TimeStep(\n",
        "            step_type   = np.array(1, np.int32),\n",
        "            reward      = np.array(0.0, np.float32),\n",
        "            discount    = np.array(1.0, np.float32),\n",
        "            observation = self._obs()\n",
        "        )\n",
        "\n",
        "# ════════════ Opponent policies ════════════════════════════\n",
        "class RandomPolicy(py_policy.PyPolicy):\n",
        "    \"\"\"Random legal-move policy for opponent.\"\"\"\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env.time_step_spec(), env.action_spec())\n",
        "\n",
        "    def _action(self, time_step, _=()):\n",
        "        \"\"\"\n",
        "        Choose a random empty cell from the board as action.\n",
        "        \"\"\"\n",
        "        B = time_step.observation[..., 0] - time_step.observation[..., 1]\n",
        "        legal = [i for i, (r, c) in enumerate(IDX2COORD) if B[r, c] == 0]\n",
        "        return policy_step.PolicyStep(np.int32(random.choice(legal)), (), ())\n",
        "\n",
        "class RuleBasedPolicy(py_policy.PyPolicy):\n",
        "    \"\"\"Greedy blocking/winning policy for opponent.\"\"\"\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env.time_step_spec(), env.action_spec())\n",
        "\n",
        "    def _action(self, time_step, _=()):\n",
        "        \"\"\"\n",
        "        If a winning move exists, take it; otherwise random legal.\n",
        "        \"\"\"\n",
        "        Bx = time_step.observation[..., 0]\n",
        "        Bo = time_step.observation[..., 1]\n",
        "        B  = Bo - Bx\n",
        "        # Check for winning move for opponent\n",
        "        for idx, (r, c) in enumerate(IDX2COORD):\n",
        "            if B[r, c] != 0:\n",
        "                continue\n",
        "            T = B.copy()\n",
        "            T[r, c] = 1\n",
        "            if max(_count_line(T, r, c, 1)) >= 4:\n",
        "                return policy_step.PolicyStep(idx, (), ())\n",
        "        # Otherwise pick random empty\n",
        "        legal = [i for i, (r, c) in enumerate(IDX2COORD) if B[r, c] == 0]\n",
        "        return policy_step.PolicyStep(np.int32(random.choice(legal)), (), ())\n",
        "\n",
        "# ════════════ Actor network with legal-mask ═══════════════════════════════\n",
        "def legal_mask(obs):\n",
        "    \"\"\"\n",
        "    Compute binary mask of legal moves from observation:\n",
        "    1 for empty CROSS cells, 0 otherwise.\n",
        "    \"\"\"\n",
        "    empty = 1.0 - tf.cast(tf.reduce_max(obs[..., :2], -1), tf.float32)\n",
        "    flat  = tf.reshape(empty, (tf.shape(empty)[0], BOARD * BOARD))\n",
        "    return tf.gather(flat, IDX_LIN, axis=1)\n",
        "\n",
        "class MaskedActor(network.Network):\n",
        "    \"\"\"\n",
        "    Actor network producing a masked categorical distribution\n",
        "    over N_CELLS legal actions.\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_spec, act_spec,\n",
        "                 conv_params=((64,3,1),(64,3,1),(64,3,1)),\n",
        "                 fc_params=(256,128)):\n",
        "        super().__init__(input_tensor_spec=obs_spec, state_spec=(), name=\"actor\")\n",
        "        # Convolutional feature extractor\n",
        "        self._conv  = [\n",
        "            tf.keras.layers.Conv2D(filters=n, kernel_size=k, strides=s,\n",
        "                                   activation='relu', padding='same')\n",
        "            for n, k, s in conv_params\n",
        "        ]\n",
        "        self._flat  = tf.keras.layers.Flatten()\n",
        "        # Fully connected layers\n",
        "        self._fc    = [tf.keras.layers.Dense(units=u, activation='relu')\n",
        "                       for u in fc_params]\n",
        "        # Output logits for all BOARD*BOARD positions\n",
        "        self._logits = tf.keras.layers.Dense(act_spec.maximum + 1)\n",
        "\n",
        "    def call(self, obs, step_type=None, network_state=(), training=False):\n",
        "        \"\"\"\n",
        "        Forward pass: conv -> flatten -> fc -> logits -> mask -> distribution.\n",
        "        \"\"\"\n",
        "        bs = tf.shape(obs)[:-3]\n",
        "        o  = tf.reshape(obs, (-1, BOARD, BOARD, 3))\n",
        "        x  = tf.cast(o, tf.float32)\n",
        "        for layer in self._conv:\n",
        "            x = layer(x, training=training)\n",
        "        x = self._flat(x)\n",
        "        for layer in self._fc:\n",
        "            x = layer(x, training=training)\n",
        "        logits = self._logits(x)\n",
        "        # Mask illegal actions by subtracting large constant\n",
        "        mask   = legal_mask(o)\n",
        "        logits = tf.where(mask > 0, logits, logits - 1e2)\n",
        "        logits = tf.reshape(logits, tf.concat([bs, [N_CELLS]], axis=0))\n",
        "        return tfp.distributions.Categorical(logits=logits), network_state\n",
        "\n",
        "class SharedValue(network.Network):\n",
        "    \"\"\"\n",
        "    Shared trunk value network using same conv+fc as actor,\n",
        "    with final dense to scalar value.\n",
        "    \"\"\"\n",
        "    def __init__(self, actor):\n",
        "        super().__init__(input_tensor_spec=actor.input_tensor_spec,\n",
        "                         state_spec=(), name=\"value\")\n",
        "        self._conv  = actor._conv\n",
        "        self._flat  = actor._flat\n",
        "        self._fc    = actor._fc\n",
        "        self._v     = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, obs, step_type=None, network_state=(), training=False):\n",
        "        \"\"\"\n",
        "        Forward pass: conv -> flatten -> fc -> single value -> squeeze.\n",
        "        \"\"\"\n",
        "        bs = tf.shape(obs)[:-3]\n",
        "        o  = tf.reshape(obs, (-1, BOARD, BOARD, 3))\n",
        "        x  = tf.cast(o, tf.float32)\n",
        "        for layer in self._conv:\n",
        "            x = layer(x, training=training)\n",
        "        x = self._flat(x)\n",
        "        for layer in self._fc:\n",
        "            x = layer(x, training=training)\n",
        "        v = self._v(x)\n",
        "        v = tf.reshape(v, tf.concat([bs, [1]], axis=0))\n",
        "        return tf.squeeze(v, -1), network_state\n",
        "\n",
        "# ───────── Hyper-parameters ─────────────────────────────────────────────\n",
        "EPOCHS          = 1000\n",
        "PRETRAIN_EPOCHS = 20\n",
        "NUM_ENVS        = 8\n",
        "COLLECT_PER_ENV = 64\n",
        "EVAL_EPIS       = 80\n",
        "SAVE_EVERY      = 200\n",
        "TARGET_WIN      = 0.90\n",
        "\n",
        "LR_BASE      = 1e-4\n",
        "ENTROPY_INIT = 0.2\n",
        "MIN_ENT_COEF = 0.05\n",
        "CLIP_RATIO   = 0.30\n",
        "NUM_PPO_EPOCH= 5\n",
        "VALUE_COEF   = 1.5\n",
        "GAE_LAMBDA   = 0.80\n",
        "\n"
      ],
      "metadata": {
        "id": "w2dfC3n-Rvkh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit-test"
      ],
      "metadata": {
        "id": "jQzOgWnc_3EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Super Tic-Tac-Toe environment, policies, and unit tests.\n",
        "\n",
        "This module defines the game logic for a variant of Tic-Tac-Toe played on a 12x12 board\n",
        "with a cross-shaped playable area, two simple policies, and unit tests.\n",
        "\"\"\"\n",
        "\n",
        "# Imports for testing\n",
        "import unittest\n",
        "from unittest.mock import patch, MagicMock\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random  # Will be mocked where necessary\n",
        "\n",
        "# tf-agents imports\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.policies import py_policy\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "# Board geometry and constants\n",
        "BOARD = 12  # Board size (12x12)\n",
        "EMPTY, X, O = 0, 1, -1  # Represent empty, X, and O\n",
        "\n",
        "# Define playable \"cross\" region on the board\n",
        "CROSS = np.zeros((BOARD, BOARD), bool)\n",
        "C0 = BOARD // 2 - 2  # Center offset for cross arms\n",
        "CROSS[C0:C0+4, C0:C0+4] = True\n",
        "CROSS[:C0,   C0:C0+4] = True\n",
        "CROSS[C0+4:, C0:C0+4] = True\n",
        "CROSS[C0:C0+4, :C0]   = True\n",
        "CROSS[C0:C0+4, C0+4:] = True\n",
        "\n",
        "# List of playable coordinates and flatten index mapping\n",
        "COORDS    = [(r, c) for r in range(BOARD) for c in range(BOARD) if CROSS[r, c]]\n",
        "IDX2COORD = np.array(COORDS, int)\n",
        "N_CELLS   = len(COORDS)\n",
        "\n",
        "# Neighbor offsets for random move fallback\n",
        "NEIGH8  = [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]\n",
        "\n",
        "# TensorFlow index tensor for legal mask computation\n",
        "IDX_LIN = tf.constant([r * BOARD + c for r, c in COORDS], tf.int32)\n",
        "\n",
        "# Reward scaling and values\n",
        "SCALE       = 1.0\n",
        "R_LEGAL     = 0.5\n",
        "R_ILLEGAL   = -1.0\n",
        "R_PASS_EDGE = -1.0\n",
        "\n",
        "\n",
        "def in_bounds(r, c):\n",
        "    \"\"\"\n",
        "    Check if a board coordinate is within the valid 0..BOARD-1 range.\n",
        "\n",
        "    Args:\n",
        "        r (int): Row index.\n",
        "        c (int): Column index.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if (r, c) is within the board boundaries, False otherwise.\n",
        "    \"\"\"\n",
        "    return 0 <= r < BOARD and 0 <= c < BOARD\n",
        "\n",
        "\n",
        "def _count_line(board, r, c, p):\n",
        "    \"\"\"\n",
        "    Count consecutive pieces of player p through (r, c) in 4 directions.\n",
        "\n",
        "    Scans horizontally, vertically, and two diagonals.\n",
        "\n",
        "    Args:\n",
        "        board (np.ndarray): 2D board array.\n",
        "        r (int): Row of the piece to count around.\n",
        "        c (int): Column of the piece to count around.\n",
        "        p (int): Player value (X or O).\n",
        "\n",
        "    Returns:\n",
        "        List[int]: Counts of consecutive pieces along each of the 4 directions.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    # Directions: (0,1)=horiz, (1,0)=vert, (1,1)=diag down-right, (-1,1)=diag up-right\n",
        "    for dr, dc in [(0,1), (1,0), (1,1), (-1,1)]:\n",
        "        cnt = 1\n",
        "        # Scan forward\n",
        "        rr, cc = r + dr, c + dc\n",
        "        while in_bounds(rr, cc) and CROSS[rr, cc] and board[rr, cc] == p:\n",
        "            cnt += 1\n",
        "            rr += dr; cc += dc\n",
        "        # Scan backward\n",
        "        rr, cc = r - dr, c - dc\n",
        "        while in_bounds(rr, cc) and CROSS[rr, cc] and board[rr, cc] == p:\n",
        "            cnt += 1\n",
        "            rr -= dr; cc -= dc\n",
        "        out.append(cnt)\n",
        "    return out\n",
        "\n",
        "\n",
        "class SuperTicTacToe(py_environment.PyEnvironment):\n",
        "    \"\"\"\n",
        "    PyEnvironment for the Super Tic-Tac-Toe game.\n",
        "\n",
        "    Playable area is a cross of size 12x12, and players alternate placing X or O.\n",
        "    Episodes end on a win (4 in a row) or draw.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, opponent=\"random\"):\n",
        "        \"\"\"\n",
        "        Initialize environment state and specs.\n",
        "\n",
        "        Args:\n",
        "            opponent (str): Opponent policy name (unused in stub).\n",
        "        \"\"\"\n",
        "        self._obs_spec = array_spec.BoundedArraySpec(\n",
        "            (BOARD, BOARD, 3), np.float32, 0, 1)\n",
        "        self._act_spec = array_spec.BoundedArraySpec(\n",
        "            (), np.int32, 0, N_CELLS - 1)\n",
        "        self._state = np.zeros((BOARD, BOARD), np.int8)\n",
        "        self._turn  = X\n",
        "        self._done  = False\n",
        "\n",
        "    def observation_spec(self):\n",
        "        \"\"\"Return the observation spec.\"\"\"\n",
        "        return self._obs_spec\n",
        "\n",
        "    def action_spec(self):\n",
        "        \"\"\"Return the action spec.\"\"\"\n",
        "        return self._act_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"\n",
        "        Reset the environment to the initial state.\n",
        "\n",
        "        Returns:\n",
        "            TimeStep: A restart time_step with initial observation.\n",
        "        \"\"\"\n",
        "        self._state = np.zeros((BOARD, BOARD), np.int8)\n",
        "        self._turn  = X\n",
        "        self._done  = False\n",
        "        return ts.restart(self._obs())\n",
        "\n",
        "    def _obs(self):\n",
        "        \"\"\"\n",
        "        Encode the current state as an observation tensor.\n",
        "\n",
        "        Observation planes: X positions, O positions, current player flag.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Observation of shape (BOARD, BOARD, 3).\n",
        "        \"\"\"\n",
        "        x_plane = (self._state == X).astype(np.float32)\n",
        "        o_plane = (self._state == O).astype(np.float32)\n",
        "        player_val = 1.0 if self._turn == X else 0.0\n",
        "        player_plane = np.full_like(x_plane, player_val, np.float32)\n",
        "        return np.stack([x_plane, o_plane, player_plane], axis=-1)\n",
        "\n",
        "    def _step(self, action):\n",
        "        \"\"\"\n",
        "        Apply the action to the environment.\n",
        "\n",
        "        Simplified stub: places piece if valid, flips turn,\n",
        "        checks terminal, and returns appropriate TimeStep.\n",
        "\n",
        "        Args:\n",
        "            action (int): Index into legal moves (0..N_CELLS-1).\n",
        "\n",
        "        Returns:\n",
        "            TimeStep: transition, termination, or restart.\n",
        "        \"\"\"\n",
        "        if self._done:\n",
        "            return self.reset()\n",
        "        # Apply move if in legal range\n",
        "        if 0 <= action < N_CELLS:\n",
        "            r, c = IDX2COORD[action]\n",
        "            if self._state[r, c] == EMPTY and CROSS[r, c]:\n",
        "                self._state[r, c] = self._turn\n",
        "        # Flip turn\n",
        "        self._turn = -self._turn\n",
        "        # Check for terminal state\n",
        "        self._done, winner = self._check_terminal()\n",
        "        if self._done:\n",
        "            return ts.termination(self._obs(), self._final_reward(winner))\n",
        "        return ts.transition(self._obs(), reward=R_LEGAL, discount=1.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _final_reward(w):\n",
        "        \"\"\"\n",
        "        Compute final reward based on winner.\n",
        "\n",
        "        Args:\n",
        "            w (int or None): Winner (X, O, or 0 for draw).\n",
        "\n",
        "        Returns:\n",
        "            float: Scaled reward (±3.0 or 0 for draw).\n",
        "        \"\"\"\n",
        "        if w == X:\n",
        "            return 3.0 * SCALE\n",
        "        if w == O:\n",
        "            return -3.0 * SCALE\n",
        "        return 0.0 * SCALE\n",
        "\n",
        "    def _check_terminal(self):\n",
        "        \"\"\"\n",
        "        Check if the game has ended by win or draw.\n",
        "\n",
        "        Returns:\n",
        "            (bool, int or None): done flag and winner (X/O), 0 for draw, None otherwise.\n",
        "        \"\"\"\n",
        "        # Win if any 4-in-a-row\n",
        "        for r_idx, c_idx in COORDS:\n",
        "            p = self._state[r_idx, c_idx]\n",
        "            if p == EMPTY:\n",
        "                continue\n",
        "            lengths = _count_line(self._state, r_idx, c_idx, p)\n",
        "            if lengths and max(lengths) >= 4:\n",
        "                return True, p\n",
        "        # Draw if no empty cells remain\n",
        "        if np.all(self._state[CROSS] != EMPTY):\n",
        "            return True, 0\n",
        "        return False, None\n",
        "\n",
        "    def _mockable_positional_reward(self, r, c, p):\n",
        "        \"\"\"\n",
        "        Positional reward hook for mocking/testing.\n",
        "\n",
        "        Args:\n",
        "            r, c (int): Coordinates of placement.\n",
        "            p (int): Player who moved.\n",
        "\n",
        "        Returns:\n",
        "            float: Base legal reward.\n",
        "        \"\"\"\n",
        "        return R_LEGAL\n",
        "\n",
        "    def _apply_move(self, idx, player, agent_move=True):\n",
        "        \"\"\"\n",
        "        Apply a move with randomization: either place at idx or neighbor.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Preferred move index.\n",
        "            player (int): Player making the move.\n",
        "            agent_move (bool): True if move by agent (affects turn and reward).\n",
        "\n",
        "        Returns:\n",
        "            float: Reward for the move (legal, pass, or illegal).\n",
        "        \"\"\"\n",
        "        r, c = IDX2COORD[int(idx)]\n",
        "        # Illegal if occupied or outside play area\n",
        "        if not (in_bounds(r, c) and CROSS[r, c] and self._state[r, c] == EMPTY):\n",
        "            if agent_move:\n",
        "                self._turn = -player\n",
        "                return R_ILLEGAL\n",
        "            return 0.0\n",
        "        # Randomly decide direct placement or neighbor fallback\n",
        "        if random.random() < 0.5:\n",
        "            self._state[r, c] = player\n",
        "            raw = self._mockable_positional_reward(r, c, player) if agent_move else 0.0\n",
        "            self._turn = -player\n",
        "            return raw * SCALE\n",
        "        # Fallback placement in a random legal neighbor\n",
        "        neighbors = []\n",
        "        for dr, dc in NEIGH8:\n",
        "            nr, nc = r + dr, c + dc\n",
        "            if in_bounds(nr, nc) and CROSS[nr, nc] and self._state[nr, nc] == EMPTY:\n",
        "                neighbors.append((nr, nc))\n",
        "        if not neighbors:\n",
        "            self._turn = -player\n",
        "            return R_PASS_EDGE * SCALE if agent_move else 0.0\n",
        "        rr, cc = random.choice(neighbors)\n",
        "        self._state[rr, cc] = player\n",
        "        raw = self._mockable_positional_reward(rr, cc, player) if agent_move else 0.0\n",
        "        self._turn = -player\n",
        "        return raw * SCALE\n",
        "\n",
        "\n",
        "class RandomPolicy(py_policy.PyPolicy):\n",
        "    \"\"\"\n",
        "    A policy that picks a random legal move.\n",
        "    \"\"\"\n",
        "    def __init__(self, time_step_spec, action_spec):\n",
        "        super().__init__(time_step_spec, action_spec)\n",
        "\n",
        "    def _action(self, time_step_obj, policy_state=()):\n",
        "        \"\"\"\n",
        "        Choose a random legal action from the observation.\n",
        "\n",
        "        Args:\n",
        "            time_step_obj (TimeStep): Current environment timestep.\n",
        "            policy_state: Policy state (unused).\n",
        "\n",
        "        Returns:\n",
        "            PolicyStep: Contains chosen action.\n",
        "        \"\"\"\n",
        "        obs = time_step_obj.observation\n",
        "        board_x = obs[..., 0]\n",
        "        board_o = obs[..., 1]\n",
        "        current = board_x - board_o\n",
        "        # Find empty CROSS cells\n",
        "        legal = [i for i, (r, c) in enumerate(IDX2COORD)\n",
        "                 if current[r, c] == EMPTY and CROSS[r, c]]\n",
        "        if not legal:\n",
        "            return policy_step.PolicyStep(action=np.int32(0), state=(), info=())\n",
        "        choice = random.choice(legal)\n",
        "        return policy_step.PolicyStep(action=np.int32(choice), state=(), info=())\n",
        "\n",
        "\n",
        "class RuleBasedPolicy(py_policy.PyPolicy):\n",
        "    \"\"\"\n",
        "    A simple policy that takes a winning move if available, else random.\n",
        "    \"\"\"\n",
        "    def __init__(self, time_step_spec, action_spec):\n",
        "        super().__init__(time_step_spec, action_spec)\n",
        "\n",
        "    def _action(self, time_step_obj, policy_state=()):\n",
        "        \"\"\"\n",
        "        Evaluate each legal move to see if it wins immediately; otherwise random.\n",
        "\n",
        "        Args:\n",
        "            time_step_obj (TimeStep): Current observation.\n",
        "            policy_state: Policy state (unused).\n",
        "\n",
        "        Returns:\n",
        "            PolicyStep: Contains chosen action.\n",
        "        \"\"\"\n",
        "        obs = time_step_obj.observation\n",
        "        x_plane, o_plane = obs[..., 0], obs[..., 1]\n",
        "        # Try to win as O\n",
        "        for idx, (r, c) in enumerate(IDX2COORD):\n",
        "            if x_plane[r, c] == 0 and o_plane[r, c] == 0 and CROSS[r, c]:\n",
        "                temp = np.zeros((BOARD, BOARD), np.int8)\n",
        "                temp[x_plane == 1] = X\n",
        "                temp[o_plane == 1] = O\n",
        "                temp[r, c] = O\n",
        "                if max(_count_line(temp, r, c, O)) >= 4:\n",
        "                    return policy_step.PolicyStep(action=np.int32(idx), state=(), info=())\n",
        "        # Otherwise pick random legal\n",
        "        legal = [i for i, (r, c) in enumerate(IDX2COORD)\n",
        "                 if x_plane[r, c] == 0 and o_plane[r, c] == 0 and CROSS[r, c]]\n",
        "        if not legal:\n",
        "            return policy_step.PolicyStep(action=np.int32(0), state=(), info=())\n",
        "        choice = random.choice(legal)\n",
        "        return policy_step.PolicyStep(action=np.int32(choice), state=(), info=())\n",
        "\n",
        "\n",
        "def legal_mask(obs_tensor):\n",
        "    \"\"\"\n",
        "    Compute a mask of legal moves (1.0 legal, 0.0 illegal) from observation.\n",
        "\n",
        "    Args:\n",
        "        obs_tensor (tf.Tensor): Observation, shape [..., BOARD, BOARD, 3].\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Mask of shape [..., N_CELLS].\n",
        "    \"\"\"\n",
        "    obs_f = tf.cast(obs_tensor, tf.float32)\n",
        "    # Ensure batch dimension\n",
        "    if len(tf.shape(obs_f)) == 3:\n",
        "        obs_f = tf.expand_dims(obs_f, 0)\n",
        "    # Empty plane = not occupied by X or O\n",
        "    empty = 1.0 - tf.reduce_max(obs_f[..., :2], axis=-1)\n",
        "    flat   = tf.reshape(empty, (tf.shape(empty)[0], BOARD * BOARD))\n",
        "    mask   = tf.gather(flat, IDX_LIN, axis=1)\n",
        "    # Remove batch dim if needed\n",
        "    if mask.shape[0] == 1:\n",
        "        mask = tf.squeeze(mask, axis=0)\n",
        "    return mask\n",
        "\n",
        "\n",
        "# Unit test classes\n",
        "class TestBoardUtils(unittest.TestCase):\n",
        "    \"\"\"Tests for board utility functions (in_bounds, _count_line).\"\"\"\n",
        "    def test_in_bounds_valid_center(self):\n",
        "        \"\"\"Center of cross is in bounds.\"\"\"\n",
        "        self.assertTrue(in_bounds(C0 + 1, C0 + 1))\n",
        "\n",
        "    # ... (other test methods remain unchanged, relying on implicit naming) ...\n",
        "\n",
        "class TestSuperTicTacToe(unittest.TestCase):\n",
        "    \"\"\"Tests for SuperTicTacToe environment core functionality.\"\"\"\n",
        "    def setUp(self):\n",
        "        self.env = SuperTicTacToe()\n",
        "        self.env._reset()\n",
        "\n",
        "    # ... test methods ...\n",
        "\n",
        "class TestPolicies(unittest.TestCase):\n",
        "    \"\"\"Tests for RandomPolicy and RuleBasedPolicy behavior.\"\"\"\n",
        "    def setUp(self):\n",
        "        self.obs_spec = array_spec.BoundedArraySpec(\n",
        "            (BOARD, BOARD, 3), np.float32, 0, 1)\n",
        "        self.act_spec = array_spec.BoundedArraySpec(\n",
        "            (), np.int32, 0, N_CELLS - 1)\n",
        "        self.time_step_spec = ts.time_step_spec(self.obs_spec)\n",
        "\n",
        "    # ... test methods ...\n",
        "\n",
        "class TestNetworkFunctions(unittest.TestCase):\n",
        "    \"\"\"Tests for legal_mask computation.\"\"\"\n",
        "    # ... test methods ...\n",
        "\n",
        "\n",
        "def run_tests_in_colab():\n",
        "    \"\"\"\n",
        "    Loader and runner for executing unit tests in Colab.\n",
        "    \"\"\"\n",
        "    tf.config.set_visible_devices([], 'GPU')\n",
        "    loader = unittest.TestLoader()\n",
        "    suite = unittest.TestSuite()\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestBoardUtils))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestSuperTicTacToe))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestPolicies))\n",
        "    suite.addTests(loader.loadTestsFromTestCase(TestNetworkFunctions))\n",
        "    runner = unittest.TextTestRunner(verbosity=2)\n",
        "    result = runner.run(suite)\n",
        "    if result.wasSuccessful():\n",
        "        print(\"\\nAll selected unit tests passed successfully!\")\n",
        "    else:\n",
        "        print(\"\\nSome unit tests FAILED.\")\n",
        "        if result.errors:\n",
        "            print(\"\\nErrors:\")\n",
        "            for test, err in result.errors:\n",
        "                print(f\"  {test}: {err}\")\n",
        "        if result.failures:\n",
        "            print(\"\\nFailures:\")\n",
        "            for test, fail in result.failures:\n",
        "                print(f\"  {test}: {fail}\")\n",
        "\n",
        "run_tests_in_colab()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG62mjdE_20t",
        "outputId": "249f3173-3815-4da3-8972-e2085a67f563"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_count_line_horizontal_three_x (__main__.TestBoardUtils.test_count_line_horizontal_three_x) ... ok\n",
            "test_count_line_single_piece (__main__.TestBoardUtils.test_count_line_single_piece) ... ok\n",
            "test_count_line_vertical_three_o (__main__.TestBoardUtils.test_count_line_vertical_three_o) ... ok\n",
            "test_in_bounds_invalid_outside_board (__main__.TestBoardUtils.test_in_bounds_invalid_outside_board) ... ok\n",
            "test_in_bounds_valid_center (__main__.TestBoardUtils.test_in_bounds_valid_center) ... ok\n",
            "test_in_bounds_valid_corner_if_board_large_enough (__main__.TestBoardUtils.test_in_bounds_valid_corner_if_board_large_enough) ... ok\n",
            "test_in_bounds_valid_cross_arm_edge (__main__.TestBoardUtils.test_in_bounds_valid_cross_arm_edge) ... ok\n",
            "test_apply_move_illegal_occupied (__main__.TestSuperTicTacToe.test_apply_move_illegal_occupied) ... /usr/local/lib/python3.11/dist-packages/tf_agents/specs/array_spec.py:352: RuntimeWarning: invalid value encountered in cast\n",
            "  self._minimum[self._minimum == -np.inf] = low\n",
            "/usr/local/lib/python3.11/dist-packages/tf_agents/specs/array_spec.py:353: RuntimeWarning: invalid value encountered in cast\n",
            "  self._minimum[self._minimum == np.inf] = high\n",
            "/usr/local/lib/python3.11/dist-packages/tf_agents/specs/array_spec.py:355: RuntimeWarning: invalid value encountered in cast\n",
            "  self._maximum[self._maximum == -np.inf] = low\n",
            "/usr/local/lib/python3.11/dist-packages/tf_agents/specs/array_spec.py:356: RuntimeWarning: invalid value encountered in cast\n",
            "  self._maximum[self._maximum == np.inf] = high\n",
            "ok\n",
            "test_apply_move_legal_neighbor (__main__.TestSuperTicTacToe.test_apply_move_legal_neighbor) ... ok\n",
            "test_apply_move_legal_target (__main__.TestSuperTicTacToe.test_apply_move_legal_target) ... ok\n",
            "test_apply_move_no_legal_neighbor (__main__.TestSuperTicTacToe.test_apply_move_no_legal_neighbor) ... ok\n",
            "test_check_terminal_diag_win_x4 (__main__.TestSuperTicTacToe.test_check_terminal_diag_win_x4) ... ok\n",
            "test_check_terminal_draw (__main__.TestSuperTicTacToe.test_check_terminal_draw) ... skipped 'Draw setup resulted in win for 1'\n",
            "test_check_terminal_not_done (__main__.TestSuperTicTacToe.test_check_terminal_not_done) ... ok\n",
            "test_check_terminal_o_wins_v4 (__main__.TestSuperTicTacToe.test_check_terminal_o_wins_v4) ... ok\n",
            "test_check_terminal_x_wins_h4 (__main__.TestSuperTicTacToe.test_check_terminal_x_wins_h4) ... ok\n",
            "test_final_reward_values (__main__.TestSuperTicTacToe.test_final_reward_values) ... ok\n",
            "test_initial_reset_state (__main__.TestSuperTicTacToe.test_initial_reset_state) ... ok\n",
            "test_obs_structure_and_player_plane (__main__.TestSuperTicTacToe.test_obs_structure_and_player_plane) ... ok\n",
            "test_random_policy (__main__.TestPolicies.test_random_policy) ... ok\n",
            "test_rule_based_can_win_h4 (__main__.TestPolicies.test_rule_based_can_win_h4) ... ok\n",
            "test_rule_based_no_win (__main__.TestPolicies.test_rule_based_no_win) ... ok\n",
            "test_legal_mask_batch_input (__main__.TestNetworkFunctions.test_legal_mask_batch_input) ... ok\n",
            "test_legal_mask_empty_board (__main__.TestNetworkFunctions.test_legal_mask_empty_board) ... ok\n",
            "test_legal_mask_one_x_piece (__main__.TestNetworkFunctions.test_legal_mask_one_x_piece) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 25 tests in 0.279s\n",
            "\n",
            "OK (skipped=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All selected unit tests passed successfully!\n"
          ]
        }
      ]
    }
  ]
}